{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't run the following Cell if you are using local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://{GITHUB_ACCESS_TOKEN}@github.com/AliMohseninejad/ganbert-classifier.git\n",
    "!rm ganbert-classifier/Codes/main.ipynb\n",
    "!cp -r ganbert-classifier/Codes/data/ ./\n",
    "!cp -r ganbert-classifier/Codes/evaluation/ ./\n",
    "!cp -r ganbert-classifier/Codes/model/ ./\n",
    "!cp -r ganbert-classifier/Codes/training/ ./\n",
    "!cp -r ganbert-classifier/Dataset/ ../\n",
    "!cp -r ganbert-classifier/Plots/ ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell only if you are using google colab.\n",
    "The dataset should be available on your google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "dataset_path = \"drive/MyDrive/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell only if you are using Kaggle. The dataset should be first uploaded to Kaggle as the \"subtaskB\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/kaggle/input/subtaskB/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell only if you are using local machine. The dataset should be in the \"Dataset\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../Dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/Courses/deep_learning/hw/deep_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from data.data_loader import generate_dataloader\n",
    "from model.bert import get_bert_model, get_tokenizer, get_bert_model_with_adapter\n",
    "from model.discriminator import Discriminator\n",
    "from model.generator1 import Generator1\n",
    "from training.train import train_vanilla_classier, train_gan\n",
    "from evaluation.test import test_vanilla_bert, test_gan_bert\n",
    "from training.visualize import plot_results, plot_results_gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "train_batch_size = 4\n",
    "val_test_batch_size = 4\n",
    "epochs = 10\n",
    "learning_rate = 5e-5\n",
    "max_size = 64\n",
    "model_name = \"bert-base-cased\"\n",
    "bert_tokenizer, bert_config = get_tokenizer(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../Plots/vanilla-bert/\"):\n",
    "    os.mkdir(path=\"../Plots/vanilla-bert/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_acc_list = []\n",
    "bert_f1_list = []\n",
    "for supervised_ratio in [0.01, 0.05, 0.10, 0.50]:\n",
    "    bert_save_path = f\"../Plots/vanilla-bert/bert_{int(100*(supervised_ratio))}sup.pth\"\n",
    "    discriminator_save_path = (\n",
    "        f\"../Plots/vanilla-bert/discriminator_{int(100*(supervised_ratio))}sup.pth\"\n",
    "    )\n",
    "    plot_save_path = f\"../Plots/vanilla-bert/\"\n",
    "\n",
    "    bert_model, _ = get_bert_model(model_name=model_name)\n",
    "    classifier = Discriminator(num_labels=6)\n",
    "\n",
    "    # Get dataloaders\n",
    "    train_dataloader, val_dataloader, test_dataloader = generate_dataloader(\n",
    "        dataset_folder_path=dataset_path,\n",
    "        unsupervised_ratio=1-supervised_ratio,\n",
    "        tokenizer=bert_tokenizer,\n",
    "        train_batch_size=train_batch_size,\n",
    "        valid_batch_size=val_test_batch_size,\n",
    "        test_batch_size=val_test_batch_size,\n",
    "        use_unsup=False,\n",
    "        max_length=max_size,\n",
    "        use_bow_dataset=False,\n",
    "        random_seed=42,\n",
    "    )\n",
    "\n",
    "    # Define optimizer\n",
    "    model_params = [v for v in bert_model.parameters()] + [\n",
    "        v for v in classifier.parameters()\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(model_params, lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    bert_model, classifier, vanilla_training_results = train_vanilla_classier(\n",
    "        transformer=bert_model,\n",
    "        classifier=classifier,\n",
    "        optimizer=optimizer,\n",
    "        epochs=epochs,\n",
    "        scheduler=None,\n",
    "        train_dataloader=train_dataloader,\n",
    "        validation_dataloader=val_dataloader,\n",
    "        transformer_path=bert_save_path,\n",
    "        classifier_path=discriminator_save_path,\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Test the model\n",
    "    test_acc, test_f1 = test_vanilla_bert(\n",
    "        bert_model_name=model_name,\n",
    "        transformer_path=bert_save_path,\n",
    "        discriminator_path=discriminator_save_path,\n",
    "        test_dataloader=test_dataloader\n",
    "    )\n",
    "    bert_acc_list.append(test_acc)\n",
    "    bert_f1_list.append(test_f1)\n",
    "    title_suffix = \" \" + str(int(100*(supervised_ratio))) + f\"% supervised\"\n",
    "    # Visualize results\n",
    "    plot_results(vanilla_training_results, title_suffix, plot_save_path)\n",
    "\n",
    "np.save(file=\"../Plots/vanilla-bert/acc.npy\", arr=np.array(bert_acc_list))\n",
    "np.save(file=\"../Plots/vanilla-bert/f1.npy\", arr=np.array(bert_f1_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert + Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "train_batch_size = 4\n",
    "val_test_batch_size = 4\n",
    "epochs = 10\n",
    "learning_rate = 5e-5\n",
    "max_size = 64\n",
    "model_name = \"bert-base-cased\"\n",
    "bert_tokenizer, bert_config = get_tokenizer(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../Plots/adapter-bert/\"):\n",
    "    os.mkdir(path=\"../Plots/adapter-bert/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_bert_acc_list = []\n",
    "adapter_bert_f1_list = []\n",
    "for supervised_ratio in [0.01, 0.05, 0.10, 0.50]:\n",
    "    bert_save_path = f\"../Plots/adapter-bert/bert_{int(100*(supervised_ratio))}sup.pth\"\n",
    "    discriminator_save_path = (\n",
    "        f\"../Plots/adapter-bert/discriminator_{int(100*(supervised_ratio))}sup.pth\"\n",
    "    )\n",
    "    plot_save_path = f\"../Plots/adapter-bert/\"\n",
    "\n",
    "    bert_model, _ = get_bert_model_with_adapter(model_name=model_name)\n",
    "    classifier = Discriminator(num_labels=6)\n",
    "\n",
    "    # Get dataloaders\n",
    "    train_dataloader, val_dataloader, test_dataloader = generate_dataloader(\n",
    "        dataset_folder_path=dataset_path,\n",
    "        unsupervised_ratio=1-supervised_ratio,\n",
    "        tokenizer=bert_tokenizer,\n",
    "        train_batch_size=train_batch_size,\n",
    "        valid_batch_size=val_test_batch_size,\n",
    "        test_batch_size=val_test_batch_size,\n",
    "        use_unsup=False,\n",
    "        max_length=max_size,\n",
    "        use_bow_dataset=False,\n",
    "        random_seed=42,\n",
    "    )\n",
    "\n",
    "    # Define optimizer\n",
    "    model_params = [v for v in bert_model.parameters()] + [\n",
    "        v for v in classifier.parameters()\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(model_params, lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    bert_model, classifier, vanilla_training_results = train_vanilla_classier(\n",
    "        transformer=bert_model,\n",
    "        classifier=classifier,\n",
    "        optimizer=optimizer,\n",
    "        epochs=epochs,\n",
    "        scheduler=None,\n",
    "        train_dataloader=train_dataloader,\n",
    "        validation_dataloader=val_dataloader,\n",
    "        transformer_path=bert_save_path,\n",
    "        classifier_path=discriminator_save_path,\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Test the model\n",
    "    test_acc, test_f1 = test_vanilla_bert(\n",
    "        bert_model_name=model_name,\n",
    "        transformer_path=bert_save_path,\n",
    "        discriminator_path=discriminator_save_path,\n",
    "        test_dataloader=test_dataloader\n",
    "    )\n",
    "\n",
    "    adapter_bert_acc_list.append(test_acc)\n",
    "    adapter_bert_f1_list.append(test_f1)\n",
    "    title_suffix = \" \" + str(int(100*(supervised_ratio))) + f\"% supervised\" \n",
    "    # Visualize results\n",
    "    plot_results(vanilla_training_results, title_suffix, plot_save_path)\n",
    "\n",
    "np.save(file=\"../Plots/adapter-bert/acc.npy\", arr=np.array(adapter_bert_acc_list))\n",
    "np.save(file=\"../Plots/adapter-bert/f1.npy\", arr=np.array(adapter_bert_f1_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN-BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "train_batch_size = 4\n",
    "val_test_batch_size = 4\n",
    "epochs = 10\n",
    "learning_rate_discriminator = 5e-5\n",
    "learning_rate_generator = 5e-5\n",
    "max_size = 64\n",
    "model_name = \"bert-base-cased\"\n",
    "bert_tokenizer, bert_config = get_tokenizer(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../Plots/generator1/\"):\n",
    "    os.mkdir(path=\"../Plots/generator1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_bert_acc_list = []\n",
    "g1_bert_f1_list = []\n",
    "for unsupervised_ratio in [0.99, 0.95, 0.90, 0.50]:\n",
    "    bert_save_path = (\n",
    "        f\"../Plots/generator1/bert_{int(100*(1-unsupervised_ratio))}sup.pth\"\n",
    "    )\n",
    "    discriminator_save_path = (\n",
    "        f\"../Plots/generator1/discriminator_{int(100*(1-unsupervised_ratio))}sup.pth\"\n",
    "    )\n",
    "    generator_save_path = (\n",
    "        f\"../Plots/generator1/generator_{int(100*(1-unsupervised_ratio))}sup.pth\"\n",
    "    )\n",
    "    plot_save_path = f\"../Plots/generator1/\"\n",
    "\n",
    "    bert_model, _ = get_bert_model(model_name=model_name)\n",
    "    classifier = Discriminator()\n",
    "    generator = Generator1()\n",
    "\n",
    "    # Get dataloaders\n",
    "    train_dataloader, val_dataloader, test_dataloader = generate_dataloader(\n",
    "        dataset_folder_path=dataset_path,\n",
    "        unsupervised_ratio=unsupervised_ratio,\n",
    "        tokenizer=bert_tokenizer,\n",
    "        train_batch_size=train_batch_size,\n",
    "        valid_batch_size=val_test_batch_size,\n",
    "        test_batch_size=val_test_batch_size,\n",
    "        max_length=max_size,\n",
    "        use_unsup=True,\n",
    "        use_bow_dataset=False,\n",
    "        random_seed=42,\n",
    "    )\n",
    "\n",
    "    # Define optimizers\n",
    "    discriminator_params = [v for v in bert_model.parameters()] + [\n",
    "        v for v in classifier.parameters()\n",
    "    ]\n",
    "    generator_params = [v for v in generator.parameters()]\n",
    "    d_optimizer = torch.optim.AdamW(\n",
    "        discriminator_params, lr=learning_rate_discriminator\n",
    "    )\n",
    "    g_optimizer = torch.optim.AdamW(generator_params, lr=learning_rate_generator)\n",
    "\n",
    "    bert_model, generator, classifier, gan1_training_results = train_gan(\n",
    "        transformer=bert_model,\n",
    "        generator=generator,\n",
    "        discriminator=classifier,\n",
    "        bow_mode=False,\n",
    "        generator_optimizer=g_optimizer,\n",
    "        discriminator_optimizer=d_optimizer,\n",
    "        epochs=epochs,\n",
    "        generator_scheduler=None,\n",
    "        discriminator_scheduler=None,\n",
    "        train_dataloader=train_dataloader,\n",
    "        validation_dataloader=val_dataloader,\n",
    "        transformer_path=bert_save_path,\n",
    "        discriminator_path=discriminator_save_path,\n",
    "        generator_path=generator_save_path,\n",
    "    )\n",
    "\n",
    "    # Test the model\n",
    "    test_acc, test_f1 = test_gan_bert(\n",
    "        bert_model_name=model_name,\n",
    "        transformer_path=bert_save_path,\n",
    "        generator_path=generator_save_path,\n",
    "        discriminator_path=discriminator_save_path,\n",
    "        bow_mode=False,\n",
    "        test_dataloader=test_dataloader\n",
    "    )\n",
    "    g1_bert_acc_list.append(test_acc)\n",
    "    g1_bert_f1_list.append(test_f1)\n",
    "\n",
    "    title_suffix = \" \" + str(int(100*(1-unsupervised_ratio))) + f\"% supervised\"\n",
    "    # Visualize results\n",
    "    plot_results_gan(gan1_training_results, title_suffix, plot_save_path)\n",
    "\n",
    "np.save(file=\"../Plots/generator1/acc.npy\", arr=np.array(g1_bert_acc_list))\n",
    "np.save(file=\"../Plots/generator1/f1.npy\", arr=np.array(g1_bert_f1_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "train_batch_size = 4\n",
    "val_test_batch_size = 4\n",
    "epochs = 10\n",
    "learning_rate_discriminator = 5e-5\n",
    "learning_rate_generator = 5e-5\n",
    "max_size = 64\n",
    "model_name = \"bert-base-cased\"\n",
    "bert_tokenizer, bert_config = get_tokenizer(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../Plots/generator2/\"):\n",
    "    os.mkdir(path=\"../Plots/generator2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2_bert_acc_list = []\n",
    "g2_bert_f1_list = []\n",
    "for unsupervised_ratio in [0.99, 0.95, 0.90, 0.50]:\n",
    "    bert_save_path = (\n",
    "        f\"../Plots/generator2/bert_{int(100*(1-unsupervised_ratio))}sup.pth\"\n",
    "    )\n",
    "    discriminator_save_path = (\n",
    "        f\"../Plots/generator2/discriminator_{int(100*(1-unsupervised_ratio))}sup.pth\"\n",
    "    )\n",
    "    generator_save_path = (\n",
    "        f\"../Plots/generator2/generator_{int(100*(1-unsupervised_ratio))}sup.pth\"\n",
    "    )\n",
    "    plot_save_path = f\"../Plots/generator2/\"\n",
    "\n",
    "    bert_model, _ = get_bert_model(model_name=model_name)\n",
    "    classifier = Discriminator()\n",
    "    generator, _ = get_bert_model(model_name=model_name)\n",
    "\n",
    "    # Get dataloaders\n",
    "    train_dataloader, val_dataloader, test_dataloader = generate_dataloader(\n",
    "        dataset_folder_path=dataset_path,\n",
    "        unsupervised_ratio=unsupervised_ratio,\n",
    "        tokenizer=bert_tokenizer,\n",
    "        train_batch_size=train_batch_size,\n",
    "        valid_batch_size=val_test_batch_size,\n",
    "        test_batch_size=val_test_batch_size,\n",
    "        max_length=max_size,\n",
    "        use_unsup=True,\n",
    "        use_bow_dataset=True,\n",
    "        random_seed=42,\n",
    "    )\n",
    "\n",
    "    # Define optimizers\n",
    "    discriminator_params = [v for v in bert_model.parameters()] + [\n",
    "        v for v in classifier.parameters()\n",
    "    ]\n",
    "    generator_params = [v for v in generator.parameters()]\n",
    "    d_optimizer = torch.optim.AdamW(\n",
    "        discriminator_params, lr=learning_rate_discriminator\n",
    "    )\n",
    "    g_optimizer = torch.optim.AdamW(generator_params, lr=learning_rate_generator)\n",
    "\n",
    "    bert_model, generator, classifier, gan2_training_results = train_gan(\n",
    "        transformer=bert_model,\n",
    "        generator=generator,\n",
    "        discriminator=classifier,\n",
    "        bow_mode=True,\n",
    "        generator_optimizer=g_optimizer,\n",
    "        discriminator_optimizer=d_optimizer,\n",
    "        epochs=epochs,\n",
    "        generator_scheduler=None,\n",
    "        discriminator_scheduler=None,\n",
    "        train_dataloader=train_dataloader,\n",
    "        validation_dataloader=val_dataloader,\n",
    "        transformer_path=bert_save_path,\n",
    "        discriminator_path=discriminator_save_path,\n",
    "        generator_path=generator_save_path,\n",
    "    )\n",
    "\n",
    "    # Test the model\n",
    "    test_acc, test_f1 = test_gan_bert(\n",
    "        bert_model_name=model_name,\n",
    "        transformer_path=bert_save_path,\n",
    "        generator_path=generator_save_path,\n",
    "        discriminator_path=discriminator_save_path,\n",
    "        bow_mode=True,\n",
    "        test_dataloader=test_dataloader\n",
    "    )\n",
    "    g2_bert_acc_list.append(test_acc)\n",
    "    g2_bert_f1_list.append(test_f1)\n",
    "\n",
    "    title_suffix = \" \" + str(int(100*(1-unsupervised_ratio))) + f\"% supervised\"\n",
    "    # Visualize results\n",
    "    plot_results_gan(gan2_training_results, title_suffix, plot_save_path)\n",
    "\n",
    "np.save(file=\"../Plots/generator2/acc.npy\", arr=np.array(g2_bert_acc_list))\n",
    "np.save(file=\"../Plots/generator2/f1.npy\", arr=np.array(g2_bert_f1_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
